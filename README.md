# CNN-Scratch ğŸ§ 

A **Convolutional Neural Network** implementation built from scratch using PyTorch tensors for computational efficiency while maintaining educational clarity.

## ğŸ“‹ Overview

This project implements a complete CNN framework without relying on high-level neural network libraries. It demonstrates the fundamental concepts of deep learning by building layers, activation functions, loss functions, and training procedures from the ground up.

## âœ¨ Features

- **ğŸ”§ Custom Layer Architecture**: Implemented from scratch
  - Convolutional layers with forward/backward propagation
  - Dense (fully connected) layers
  - Activation layers (Sigmoid)
  - Reshape layers for transitioning between conv and dense layers

- **ğŸ“Š Loss Functions**: 
  - Cross-entropy for classification
  - Mean Squared Error (MSE) for regression

- **ğŸ¯ Training Examples**:
  - MNIST digit classification
  - XOR problem solving

- **âš¡ GPU Support**: CUDA acceleration when available

## ğŸ—ï¸ Project Structure

```
CNN-Scratch/
â”œâ”€â”€ network/
â”‚   â”œâ”€â”€ __init__.py          # Package exports
â”‚   â”œâ”€â”€ network.py           # Main Network class
â”‚   â”œâ”€â”€ layer.py             # Base Layer class
â”‚   â”œâ”€â”€ convolutional.py     # Convolutional layer implementation
â”‚   â”œâ”€â”€ dense.py             # Dense layer implementation
â”‚   â”œâ”€â”€ activation.py        # Activation layer wrapper
â”‚   â”œâ”€â”€ activations.py       # Activation functions (sigmoid, etc.)
â”‚   â”œâ”€â”€ losses.py            # Loss functions
â”‚   â””â”€â”€ reshape.py           # Reshape layer
â”œâ”€â”€ mnist.py                 # MNIST classification example
â”œâ”€â”€ xor.py                   # XOR problem example
â””â”€â”€ data/                    # Dataset storage
```

## ğŸš€ Installation

1. **Clone the repository**:
```bash
git clone https://github.com/yourusername/CNN-Scratch.git
cd CNN-Scratch
```

2. **Install dependencies**:
```bash
pip install torch torchvision numpy scipy
```

## ğŸ“š Usage

### MNIST Classification

Train a CNN on the MNIST dataset:

```bash
python mnist.py
```

The network architecture includes:
- Convolutional layer (1â†’5 filters, 3x3 kernels)
- Sigmoid activation
- Reshape layer (flatten for dense layers)
- Dense layer (3380â†’100 neurons)
- Dense output layer (100â†’10 classes)

### XOR Problem

Solve the classic XOR problem with a simple neural network:

```bash
python xor.py
```

## ğŸ”¬ Network Architecture

### Key Components

**Dense Layer**:
- **Forward pass**: 
  $$Y = W \cdot X + B$$

- **Backward pass**:
  1) $$\frac{\partial E}{\partial W} = \frac{\partial E}{\partial Y} \cdot X^T$$
  2) $$\frac{\partial E}{\partial X} = W^T \cdot \frac{\partial E}{\partial Y}$$
  3) $$\frac{\partial E}{\partial B} = \frac{\partial E}{\partial Y}$$

**Activation Layer**:
- **Forward pass**: 
  $$Y = f(X)$$

- **Backward pass**: 
  $$\frac{\partial E}{\partial X} = \frac{\partial E}{\partial Y} \odot f'(X)$$

**Convolutional Layer**:
- **Forward pass**: 
  $$Y_i = B_i + \sum_{j=0}^{n} X_j \star K_{ij}, \quad i = 0..d$$

- **Backward pass**:
  1) $$\frac{\partial E}{\partial K_{ij}} = X_j \star \frac{\partial E}{\partial Y_i}$$
  2) $$\frac{\partial E}{\partial B} = \frac{\partial E}{\partial Y}$$
  3) $$\frac{\partial E}{\partial X_j} = \sum_{i=0}^{n} \frac{\partial E}{\partial Y_i} \underset{\text{full}}{*} K_{ij}$$

**Training Loop**:
- Forward propagation through all layers
- Loss computation
- Backward propagation with gradient descent
- Parameter updates with learning rate

## ğŸ“ Loss Functions & Activation Functions

### Loss Functions

**Mean Squared Error (MSE)**:
- **Forward**: 
  $$\text{MSE}(y_{true}, y_{pred}) = \frac{1}{n}\sum_{i=1}^{n}(y_{true} - y_{pred})^2$$

- **Derivative**: 
  $$\frac{\partial \text{MSE}}{\partial y_{pred}} = \frac{2(y_{pred} - y_{true})}{n}$$

**Cross Entropy**:
- **Forward**: 
  $$\text{CE}(y_{true}, y_{pred}) = -\sum_{i} [y_{true} \log(y_{pred}) + (1 - y_{true}) \log(1 - y_{pred})]$$

- **Derivative**: 
  $$\frac{\partial \text{CE}}{\partial y_{pred}} = \frac{1}{n}\left[\frac{1 - y_{true}}{1 - y_{pred}} - \frac{y_{true}}{y_{pred}}\right]$$

### Activation Functions

**Sigmoid**:
- **Forward**: 
  $$\sigma(x) = \frac{1}{1 + e^{-x}}$$

- **Derivative**: 
  $$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

**Tanh**:
- **Forward**: 
  $$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

- **Derivative**: 
  $$\tanh'(x) = 1 - \tanh^2(x)$$

## ğŸ“Š Example Results

### MNIST Performance
- Training epochs: 1 (configurable)
- Learning rate: 0.1
- Architecture: Conv â†’ Sigmoid â†’ Reshape â†’ Dense â†’ Sigmoid â†’ Dense â†’ Sigmoid

### XOR Performance
- Training epochs: 10,000
- Learning rate: 0.1
- Perfect convergence on XOR truth table

## ğŸ› ï¸ Customization

You can easily modify the network by:

1. **Adding new layer types** in the `network/` directory
2. **Implementing new activation functions** in `activations.py`
3. **Creating custom loss functions** in `losses.py`
4. **Adjusting hyperparameters** in the example files

## ğŸ“ Educational Value

This implementation focuses on:
- Understanding CNN mathematics and operations
- Gradient computation and backpropagation
- Layer-by-layer neural network construction
- Transition from convolutional to dense layers
- GPU acceleration with PyTorch tensors

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- Add new layer types
- Implement additional activation functions
- Improve documentation
- Add more examples

## ğŸ‘¨â€ğŸ’» Author

**Gabriel Nery** - [GitHub Profile](https://github.com/yourusername)

---

*Built with â¤ï¸ for educational purposes and deep learning enthusiasts*

### ğ˜–ğ˜®ğ˜¯ğ˜¦ğ˜´ ğ˜¦ğ˜¯ğ˜ªğ˜® ğ˜Šğ˜©ğ˜³ğ˜ªğ˜´ğ˜µğ˜¶ğ˜´, ğ˜¯ğ˜ªğ˜©ğ˜ªğ˜­ ğ˜´ğ˜ªğ˜¯ğ˜¦ ğ˜”ğ˜¢ğ˜³ğ˜ªğ˜¢ 